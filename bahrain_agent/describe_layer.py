# -*- coding: utf-8 -*-
"""describe_layer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ric0LVcuLkYbUAIaFxT6e0BCNAE8WRjZ
"""

"""
describe_layer.py

Turns DataRepository + query_layer outputs into human-readable text.
No direct CSV access here, only structured data from query_layer.
"""
import re
import pandas as pd
from typing import Optional, Dict, Any
from textwrap import indent
from datetime import datetime
from .utils.io import load_csv_safe
from .data_layer import DataRepository

# bring query_layer helpers and new query functions
from .query_layer import (
    get_workers_by_year,
    get_top_occupations,
    get_households_by_governorate,
    get_latest_density,
    get_housing_units_summary,
    get_students_summary,
    get_teachers_summary,
    get_higher_education_summary,
    # newly added functions
    get_domestic_permit_stats,
    get_employer_household_stats,
    get_work_permit_applications,
    get_permit_renewals,
    get_workforce_distribution,
    get_salary_fee_aggregates,
    get_footfall_summary,
    safe_get_df,
    ensure_numeric_series,
)


def short_pct(v, total):
    try:
        return f"{100*v/total:.1f}%"
    except:
        return "0%"


def summarize_df_by(df, groupby_cols, value_col=None, top_n=5):
    if value_col:
        agg = df.groupby(groupby_cols)[value_col].sum().reset_index()
    else:
        agg = df.groupby(groupby_cols).size().reset_index(name="count")
    agg = agg.sort_values(by=agg.columns[-1], ascending=False).head(top_n)
    return agg


def extract_year_from_series(s: pd.Series) -> int | None:
    """
    Robustly extract an integer year from a Series that may contain:
      - integers (2023)
      - numeric strings ("2023")
      - year-month ("2023-12")
      - full dates ("2023-12-01" or other parseable formats)
    Returns max year found or None.
    """
    if s is None or s.empty:
        return None

    # 1. Try numeric conversion (fast path)
    try:
        nums = pd.to_numeric(s, errors="coerce")
        if nums.notna().any():
            years = nums.dropna().astype(int)
            if len(years) > 0:
                return int(years.max())
    except Exception:
        pass

    # 2. Parse strings like "YYYY", "YYYY-MM", "YYYY-MM-DD"
    def parse_year_from_string(val):
        if not isinstance(val, str):
            return None
        v = val.strip()
        m = re.match(r"^(\d{4})(?:[-/].*)?$", v)
        if m:
            return int(m.group(1))
        return None

    parsed = []
    for v in pd.Series(s).dropna().unique():
        try:
            y = parse_year_from_string(v)
            if y:
                parsed.append(y)
        except Exception:
            continue
    if parsed:
        return int(max(parsed))

    # 3. Fallback: use pandas to_datetime and extract year
    try:
        dt = pd.to_datetime(s, errors="coerce", utc=True)
        if dt.notna().any():
            return int(dt.dt.year.max())
    except Exception:
        pass

    return None


# ---------------------------
# Helpers used by describers
# ---------------------------
def normalize_external_fields(df: pd.DataFrame) -> pd.DataFrame:
    """
    Lightweight normalization to friendly column names the describers expect:
    - lower-case column names with underscores
    - common alias mapping applied (e.g., 'avg_dwell_time_seconds' or 'avg_dwell')
    """
    if df is None:
        return pd.DataFrame()
    if df.empty:
        return df.copy()

    df = df.copy()
    # normalize column names
    cols = {c: re.sub(r"\s+|[^\w]", "_", c.strip().lower()) for c in df.columns}
    df.rename(columns=cols, inplace=True)

    # alias map (extend as needed)
    alias_map = {
        "avg_dwell": "avg_dwell_time_seconds",
        "dwell_time": "avg_dwell_time_seconds",
        "visits": "footfall_visits",
        "count": "footfall_count",
        "devices": "footfall_unique_devices",
        "area": "area_name",
        "area_code": "area_code",
        "occupation": "job_role",
        "occupation_workers": "occupation_workers",
    }
    for k, v in alias_map.items():
        if k in df.columns and v not in df.columns:
            df.rename(columns={k: v}, inplace=True)
    return df


def summarize_external_fields(df: pd.DataFrame) -> Dict[str, Any]:
    """
    Lightweight classifier that returns which families of fields are present.
    e.g. {'mobility': True, 'labour': True}
    """
    out: Dict[str, Any] = {}
    if df is None or df.empty:
        return out
    cols = set([c.lower() for c in df.columns])
    out["mobility"] = bool(cols & {"footfall_visits", "footfall_unique_devices", "avg_dwell_time_seconds", "movement_direction"})
    out["labour"] = bool(cols & {"active_permits", "new_permits", "avg_salary", "occupation_workers", "total_workers"})
    out["salary"] = bool(cols & {"salary", "avg_salary", "recruitment_fee", "recruitment_fees"})
    return out


def unwrap_sectioned_df(df: pd.DataFrame, section: str) -> pd.DataFrame:
    """
    If df contains a _section column, return only rows for that section with _section dropped.
    Otherwise return df unchanged.
    """
    if df is None or df.empty:
        return pd.DataFrame()
    if "_section" not in df.columns:
        return df
    out = df[df["_section"] == section].copy()
    if "_section" in out.columns:
        out.drop(columns=["_section"], inplace=True)
    return out


# ---------------------------
# Describe functions
# ---------------------------


def describe_labour_market(repo: DataRepository, year: Optional[int]) -> str:
    workers = get_workers_by_year(repo, year)
    top_occ = get_top_occupations(repo, year, top_n=3)

    if workers.empty and top_occ.empty:
        if year is None:
            return "I couldn't find any labour market data in the current dataset."
        return f"I couldn't find labour market data for {year} in the current dataset."

    lines = []
    if year is not None:
        lines.append(f"Labour market overview for {year} in Bahrain:\n")
    else:
        lines.append("Labour market overview for Bahrain:\n")

    if not workers.empty:
        # ensure column name exists
        if "total_workers" in workers.columns:
            total = int(pd.to_numeric(workers["total_workers"], errors="coerce").sum())
            lines.append(f"- Total recorded workers: {total:,}")
        else:
            # fallback: sum any numeric column
            numeric_cols = [c for c in workers.columns if pd.api.types.is_numeric_dtype(workers[c])]
            if numeric_cols:
                total = int(pd.to_numeric(workers[numeric_cols[0]], errors="coerce").sum())
                lines.append(f"- Total recorded workers (inferred): {total:,}")

        try:
            top_nat = workers.iloc[0]
            if "nationality" in workers.columns and "total_workers" in workers.columns:
                lines.append(
                    f"- Largest nationality group: {top_nat['nationality']} "
                    f"with {int(top_nat['total_workers']):,} workers"
                )
        except Exception:
            pass

    if not top_occ.empty:
        lines.append("\nTop 3 occupations by number of workers:")
        for _, row in top_occ.iterrows():
            occ = row.get("main_occupation", row.index[0] if len(row.index) else "occupation")
            # pick numeric column to display
            vals = [v for v in row.index if pd.api.types.is_numeric_dtype(top_occ[v])]
            if vals:
                count_val = int(pd.to_numeric(row[vals[0]], errors="coerce") or 0)
            else:
                # second column fallback
                count_val = int(row.iloc[1]) if len(row) > 1 and pd.api.types.is_numeric_dtype(type(row.iloc[1])) else 0
            lines.append(f"- {occ}: {count_val:,} workers")

    return "\n".join(lines)


def describe_governorates(repo: DataRepository) -> str:
    households = get_households_by_governorate(repo)
    density = get_latest_density(repo)

    if (households is None or households.empty) and (density is None or density.empty):
        return "I couldn't find households or population density data in the current dataset."

    lines = []
    lines.append("Households and population density by governorate:\n")

    if households is not None and not households.empty:
        lines.append("Households by governorate:")
        # Show top governorates
        top = households.head(10)
        for _, row in top.iterrows():
            gov = row.get("governorate", "Unknown governorate")
            # assume last numeric column is value
            val = row.iloc[-1]
            nat = row.get("nationality")
            try:
                val_int = int(pd.to_numeric(val, errors="coerce") or 0)
                if nat:
                    lines.append(f"- {gov} ({nat}): {val_int:,} households")
                else:
                    lines.append(f"- {gov}: {val_int:,} households")
            except Exception:
                if nat:
                    lines.append(f"- {gov} ({nat}): {val} households")
                else:
                    lines.append(f"- {gov}: {val} households")

    if density is not None and not density.empty:
        latest_year = extract_year_from_series(density["year"]) if "year" in density.columns else None
        if latest_year:
            lines.append(f"\nLatest population density data (year {latest_year}):")
        else:
            lines.append("\nLatest population density data:")

        for _, row in density.iterrows():
            gov = row.get("governorate", "Unknown governorate")
            pop = row.get("population")
            dens = row.get("density")
            try:
                if pd.notna(pop) and pd.notna(dens):
                    lines.append(f"- {gov}: population {int(pd.to_numeric(pop, errors='coerce')):,}, density {dens} people/km²")
                elif pd.notna(pop):
                    lines.append(f"- {gov}: population {int(pd.to_numeric(pop, errors='coerce')):,}")
                else:
                    lines.append(f"- {gov}: density data available")
            except Exception:
                lines.append(f"- {gov}: density data present")

    return "\n".join(lines)


def describe_housing_units(repo: DataRepository) -> str:
    df = get_housing_units_summary(repo)
    if df is None or df.empty:
        return "I couldn't find housing units data in the current dataset."

    lines = ["Housing units by governorate and type:\n"]
    top = df.head(15)
    for _, row in top.iterrows():
        gov = row.get("governorate", "Unknown governorate")
        htype = row.get("housing_type", "All types")
        units = row.iloc[-1]
        try:
            units_int = int(pd.to_numeric(units, errors="coerce") or 0)
            lines.append(f"- {gov} – {htype}: {units_int:,} units")
        except Exception:
            lines.append(f"- {gov} – {htype}: {units} units")

    return "\n".join(lines)


def describe_students(repo_or_path="data/bahrain_master"):
    """
    Prefer using DataRepository -> query_layer.get_students_summary when a repo object is provided.
    Otherwise fall back to loading students.csv from disk.
    """
    if isinstance(repo_or_path, DataRepository):
        df = safe_get_df(repo_or_path, "students")
        if df is None or df.empty:
            return "No `students` layer found in DataRepository."
        df = normalize_external_fields(df)
    else:
        repo_root = getattr(repo_or_path, "data_dir", None) or getattr(repo_or_path, "data_root", None) or getattr(repo_or_path, "root_path", None) or getattr(repo_or_path, "root", None) or getattr(repo_or_path, "path", None) or str(repo_or_path)
        df, err = load_csv_safe(repo_root, "students.csv")
        if err or df is None:
            return "No students CSV found. Expected file: students.csv in data/bahrain_master."
        df.columns = [c.strip() for c in df.columns]
        df = normalize_external_fields(df)

    out = []
    out.append(f"Students dataset — {len(df)} rows, columns: {', '.join(df.columns)}")
    # totals if column exists
    if "students" in df.columns:
        total = int(pd.to_numeric(df["students"], errors="coerce").sum() or 0)
        out.append(f"Total students: {total}")
    # groupings examples
    if "governorate" in df.columns:
        top = df.groupby("governorate").agg({"students":"sum"})["students"].sort_values(ascending=False).head(5)
        out.append("Top governorates by students: " + ", ".join([f"{i} ({int(v)})" for i, v in top.items()]))
    return "\n".join(out)


def describe_teachers(repo: DataRepository) -> str:
    df = get_teachers_summary(repo)
    if df is None or df.empty:
        return "I couldn't find teachers data in the current dataset."

    lines = ["Teachers overview by school type and level:\n"]
    top = df.head(15)
    value_col = "teachers"
    for _, row in top.iterrows():
        desc_parts = []
        for c in ["school_type", "sector"]:
            if c in row and isinstance(row[c], str):
                desc_parts.append(row[c])
        level = row.get("level", "All levels")
        count = int(pd.to_numeric(row[value_col], errors="coerce") or 0)
        if desc_parts:
            lines.append(f"- {' / '.join(desc_parts)} – {level}: {count:,} teachers")
        else:
            lines.append(f"- {level}: {count:,} teachers")

    return "\n".join(lines)


def describe_higher_education(repo_or_path="data/bahrain_master"):
    """
    Prefer DataRepository/higher_education layer when available, else fall back to CSV.
    """
    if isinstance(repo_or_path, DataRepository):
        df = safe_get_df(repo_or_path, "higher_education")
        if df is None or df.empty:
            return "No higher_education layer found in DataRepository."
        df = normalize_external_fields(df)
    else:
        df, err = load_csv_safe(repo_or_path, "higher_education.csv")
        if err or df is None:
            return ("No higher education CSV found. Expected file: higher_education.csv. "
                    "Files present: check data/bahrain_master/ directory.")
        df.columns = [c.strip() for c in df.columns]
        df = normalize_external_fields(df)

    out = []
    out.append(f"Higher education — quick overview (rows: {len(df)}).")
    total_students = None
    if "students" in df.columns:
        total_students = int(pd.to_numeric(df["students"], errors="coerce").sum() or 0)
    elif "count" in df.columns:
        total_students = int(pd.to_numeric(df["count"], errors="coerce").sum() or 0)
    if total_students:
        out.append(f"Total students (sum of 'students'/'count'): {total_students}")
    if "governorate" in df.columns:
        by_gov = df.groupby("governorate")["students"].sum() if "students" in df.columns else df.groupby("governorate").size()
        by_gov = by_gov.sort_values(ascending=False)
        out.append("Top governorates by students: " + ", ".join([f"{i} ({int(v)})" for i, v in by_gov.head(5).items()]))
    if any("gender" in c for c in df.columns):
        gcol = [c for c in df.columns if "gender" in c][0]
        if "students" in df.columns:
            gdist = df.groupby(gcol)["students"].sum()
        else:
            gdist = df.groupby(gcol).size()
        tot = gdist.sum()
        out.append("Gender split: " + ", ".join([f"{g}: {int(v)} ({short_pct(v, tot)})" for g, v in gdist.items()]))
    if any("year" in c for c in df.columns):
        ycol = [c for c in df.columns if "year" in c][0]
        years = sorted(df[ycol].dropna().unique().tolist())
        if years:
            out.append(f"Years available: {years[0]} to {years[-1]}")
    return "\n".join(out)


def describe_labour_enriched(repo_or_path="data/bahrain_master", year: int | None = None) -> str:
    """
    Enriched labour description inspired by LMRA-style fields.
    Prefers structured query_layer outputs when a DataRepository is supplied (non-destructive).
    """
    # If a DataRepository is provided, prefer the query_layer consolidated function
    if isinstance(repo_or_path, DataRepository):
        repo_obj = repo_or_path
        df = get_domestic_permit_stats(repo_obj, year)
        # get_domestic_permit_stats can return combined frames with _section tags; prefer 'by_area' if present
        if df is None or df.empty:
            # fallback to labour_master
            dfl = safe_get_df(repo_obj, "labour_master")
            df = dfl.copy() if not dfl.empty else pd.DataFrame()
        else:
            # if multi-section, prefer to use original dataframe for numeric summaries
            # but our summarizers below expect a flat DataFrame; attempt to unwrap a main section
            # try numeric_summary first
            num_summary = unwrap_sectioned_df(df, "numeric_summary")
            if not num_summary.empty:
                # numeric_summary may be key:value; we still prefer the original df for other summaries
                dfl = safe_get_df(repo_obj, "labour_master")
                df = dfl.copy() if not dfl.empty else df
            else:
                # attempt to unwrap 'by_area' or use df as-is
                u = unwrap_sectioned_df(df, "by_area")
                df = u if not u.empty else df
    else:
        # path-based fallback: load CSVs like before
        repo_root = getattr(repo_or_path, "data_dir", None) or getattr(repo_or_path, "data_root", None) or getattr(repo_or_path, "root_path", None) or getattr(repo_or_path, "root", None) or getattr(repo_or_path, "path", None) or str(repo_or_path)
        df, err = load_csv_safe(repo_root, "labour_master.csv")
        if err or df is None:
            df_alt, err2 = load_csv_safe(repo_root, "occupation_workers.csv")
            if err2 or df_alt is None:
                return "No labour CSV found (expected labour_master.csv or occupation_workers.csv)."
            df = df_alt
        df = normalize_external_fields(df)

    # ensure normalized
    dfn = normalize_external_fields(df) if df is not None else pd.DataFrame()

    lines = []
    lines.append("Labour market enriched summary:")

    # year filtering if requested and 'year' or 'period' exists
    if year is not None:
        if "year" in dfn.columns:
            dsub = dfn[dfn["year"] == int(year)]
        elif "period" in dfn.columns:
            dsub = dfn[dfn["period"].astype(str).str.contains(str(year))]
        else:
            dsub = dfn
    else:
        dsub = dfn

    # totals
    if "active_permits" in dsub.columns:
        try:
            total_active = int(pd.to_numeric(dsub["active_permits"], errors="coerce").sum(skipna=True) or 0)
            lines.append(f"- Total active permits (dataset): {total_active:,}")
        except Exception:
            pass

    # changes/new/renewed/cancelled if present
    changes = []
    for c in ("new_permits", "renewed_permits", "cancelled_permits"):
        if c in dsub.columns:
            try:
                val = int(pd.to_numeric(dsub[c], errors="coerce").sum(skipna=True) or 0)
                changes.append(f"{c.replace('_',' ')}: {val:,}")
            except Exception:
                continue
    if changes:
        lines.append("- Permit changes: " + "; ".join(changes))

    # by governorate
    if "governorate" in dsub.columns and "active_permits" in dsub.columns:
        try:
            grp = dsub.groupby("governorate")["active_permits"].sum().sort_values(ascending=False).head(5)
            lines.append("- Top governorates by active permits: " + ", ".join([f"{g} ({int(v):,})" for g, v in grp.items()]))
        except Exception:
            pass

    # by nationality
    if "nationality_group" in dsub.columns and "active_permits" in dsub.columns:
        try:
            nat = dsub.groupby("nationality_group")["active_permits"].sum().sort_values(ascending=False).head(5)
            lines.append("- Top nationalities by active permits: " + ", ".join([f"{n} ({int(v):,})" for n, v in nat.items()]))
        except Exception:
            pass

    # salary summary if present
    if "avg_salary" in dsub.columns:
        try:
            s = pd.to_numeric(dsub["avg_salary"], errors="coerce")
            lines.append(f"- Avg salary (mean): {s.mean():,.2f} ; median: {s.median():,.2f}")
        except Exception:
            pass

    # household hiring if present
    if "households_hiring" in dsub.columns:
        try:
            hh = int(pd.to_numeric(dsub["households_hiring"], errors="coerce").sum(skipna=True) or 0)
            lines.append(f"- Households hiring (total): {hh:,}")
        except Exception:
            pass

    # time series hint
    if "period" in dsub.columns and "active_permits" in dsub.columns:
        try:
            ts = dsub.copy()
            ts["period"] = pd.to_datetime(ts["period"], errors="coerce")
            period_summary = ts.groupby(pd.Grouper(key="period", freq="M"))["active_permits"].sum().dropna()
            if not period_summary.empty:
                last_idx = period_summary.index.max()
                last_val = int(period_summary.iloc[-1])
                lines.append(f"- Latest month ({last_idx.strftime('%Y-%m')}): {last_val:,} active permits")
        except Exception:
            pass

    # attach extra-fields short note (if you want)
    try:
        extra = summarize_external_fields(df)
        if extra.get("labour"):
            lines.append("- Note: additional labour insights available programmatically.")
    except Exception:
        pass

    return "\n".join(lines)


def describe_mobility_overview(repo_or_path="data/bahrain_master") -> str:
    """
    Mobility overview (Batelco-style fields).
    Prefers query_layer.get_footfall_summary when a DataRepository is provided.
    Falls back to scanning CSV files in the path otherwise.
    """
    # If repo object passed, prefer query_layer
    if isinstance(repo_or_path, DataRepository):
        df = get_footfall_summary(repo_or_path)
        if df is None or df.empty:
            return "No mobility-like data found in the DataRepository."
        # normalize if necessary
        dfn = normalize_external_fields(df)
    else:
        repo_root = getattr(repo_or_path, "data_dir", None) or getattr(repo_or_path, "data_root", None) or getattr(repo_or_path, "root_path", None) or getattr(repo_or_path, "root", None) or getattr(repo_or_path, "path", None) or str(repo_or_path)
        # mobility could be in a separate incoming file; we attempt to read a few likely masters
        df_candidates = []
        for fname in ("mobility.csv", "footfall.csv", "higher_education.csv", "population_density.csv"):
            d, e = load_csv_safe(repo_root, fname)
            if d is not None and e is None and not d.empty:
                df_candidates.append(d)
        if not df_candidates:
            # fallback scan
            for fname in ("students.csv", "higher_education.csv", "population_density.csv", "housing_units.csv"):
                d, e = load_csv_safe(repo_root, fname)
                if d is None or e:
                    continue
                cols = [c.strip().lower() for c in d.columns]
                if any(k in " ".join(cols) for k in ["footfall", "device", "dwell", "visits", "area"]):
                    df_candidates.append(d)
        if not df_candidates:
            return "No mobility-like data found in the master dataset."
        df = df_candidates[0]
        df.columns = [c.strip() for c in df.columns]
        dfn = normalize_external_fields(df)

    lines = []
    lines.append("Mobility / footfall overview:")

    # totals
    if "footfall_unique_devices" in dfn.columns:
        try:
            total_devices = int(pd.to_numeric(dfn["footfall_unique_devices"], errors="coerce").sum(skipna=True) or 0)
            lines.append(f"- Total unique devices observed: {total_devices:,}")
        except Exception:
            pass
    if "footfall_visits" in dfn.columns:
        try:
            total_visits = int(pd.to_numeric(dfn["footfall_visits"], errors="coerce").sum(skipna=True) or 0)
            lines.append(f"- Total visits recorded: {total_visits:,}")
        except Exception:
            pass

    # average dwell if present
    if "avg_dwell_time_seconds" in dfn.columns:
        try:
            avg = float(pd.to_numeric(dfn["avg_dwell_time_seconds"], errors="coerce").mean())
            lines.append(f"- Average dwell time: {avg:.1f} seconds")
        except Exception:
            pass

    # top areas by visits/devices
    if "area_name" in dfn.columns and ("footfall_visits" in dfn.columns or "footfall_unique_devices" in dfn.columns):
        try:
            metric = "footfall_visits" if "footfall_visits" in dfn.columns else "footfall_unique_devices"
            grp = dfn.groupby("area_name")[metric].sum().sort_values(ascending=False).head(10)
            lines.append("- Top areas by activity: " + ", ".join([f"{a} ({int(v):,})" for a, v in grp.items()]))
        except Exception:
            pass

    # date range
    if "date" in dfn.columns:
        try:
            dmin = dfn["date"].min()
            dmax = dfn["date"].max()
            lines.append(f"- Data covers: {pd.to_datetime(dmin).date()} to {pd.to_datetime(dmax).date()}")
        except Exception:
            pass

    # origin/destination summary if present
    if "origin_area_code" in dfn.columns or "destination_area_code" in dfn.columns:
        try:
            od_lines = []
            if "origin_area_code" in dfn.columns:
                top_o = dfn.groupby("origin_area_code").size().sort_values(ascending=False).head(5)
                od_lines.append("top origins: " + ", ".join([f"{o} ({int(v)})" for o, v in top_o.items()]))
            if "destination_area_code" in dfn.columns:
                top_d = dfn.groupby("destination_area_code").size().sort_values(ascending=False).head(5)
                od_lines.append("top destinations: " + ", ".join([f"{o} ({int(v)})" for o, v in top_d.items()]))
            if od_lines:
                lines.append("- " + " ; ".join(od_lines))
        except Exception:
            pass

    # attach brief note if more numeric columns exist
    try:
        extra = summarize_external_fields(df)
        if extra.get("mobility"):
            lines.append("- Note: mobility details available programmatically.")
    except Exception:
        pass

    return "\n".join(lines)
