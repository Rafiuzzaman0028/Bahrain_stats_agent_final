# -*- coding: utf-8 -*-
"""query_layer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ric0LVcuLkYbUAIaFxT6e0BCNAE8WRjZ
"""

"""
query_layer.py

Low-level analytical functions that operate on DataRepository
and return DataFrames (or similar structured objects), NOT text.

These functions are used by the describe_layer.
"""

from typing import Optional, Dict, Any, List
import logging
import pandas as pd

from .data_layer import DataRepository

# Add near top of query_layer.py
# make sure to import load_csv_safe used below
from .utils.io import load_csv_safe  # adjust path as needed

logger = logging.getLogger(__name__)


def safe_get_df(repo, attr: str) -> pd.DataFrame:
    """Return repo.<attr> DataFrame or empty DataFrame if missing/None."""
    try:
        df = getattr(repo, attr)
        if df is None:
            return pd.DataFrame()
        if not isinstance(df, pd.DataFrame):
            return pd.DataFrame()
        return df
    except Exception:
        return pd.DataFrame()


def ensure_numeric_series(s: pd.Series) -> pd.Series:
    """Return numeric-converted series (coerce errors to NaN)."""
    return pd.to_numeric(s, errors="coerce")


def available_repo_layers(repo) -> List[str]:
    """Return list of DataFrame-like attributes in repo (heuristic)."""
    attrs = []
    for name in dir(repo):
        if name.startswith("_"):
            continue
        try:
            val = getattr(repo, name)
        except Exception:
            continue
        if isinstance(val, pd.DataFrame):
            attrs.append(name)
    return attrs


def _safe_filter_year(df: pd.DataFrame, year: Optional[int]) -> pd.DataFrame:
    if df.empty or year is None:
        return df
    # try 'year' or any year-like column
    if "year" in df.columns:
        return df[df["year"] == year]
    # check other possible names
    for cand in ["yr", "reporting_year"]:
        if cand in df.columns:
            return df[df[cand] == year]
    return df


# ---------- QUERY: extra fields summary for machine use ----------

# ensure load_csv_safe is imported in this module (above)
def query_extra_fields_for_file(repo, fname: str) -> Dict[str, Any]:
    """
    Loads fname via load_csv_safe(repo, fname) and returns JSON-ready
    summaries for any extra / new fields found.
    """
    df, err = load_csv_safe(repo, fname)
    if err or df is None:
        return {"error": err or "file_missing"}
    result: Dict[str, Any] = {}

    # numeric column summaries (auto-adaptive)
    numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
    numeric_summary = {}
    for c in numeric_cols:
        try:
            s = pd.to_numeric(df[c], errors="coerce")
            numeric_summary[c] = {
                "count": int(s.count()),
                "min": float(s.min()) if s.count() else None,
                "max": float(s.max()) if s.count() else None,
                "mean": float(s.mean()) if s.count() else None,
            }
        except Exception:
            continue
    if numeric_summary:
        result["numeric_columns"] = numeric_summary

    # simple presence map of important optional fields
    optional_fields = [
        "avg_salary",
        "active_permits",
        "footfall_visits",
        "footfall_unique_devices",
        "governorate",
        "area_name",
        "period",
    ]
    presence = {f: (f in df.columns) for f in optional_fields}
    result["presence"] = presence

    return result


# --------------------------------------------------------------------
# ---------- Labour & Occupations ----------
# (existing functions retained; made non-destructive where appropriate)
# --------------------------------------------------------------------


def get_workers_by_year(repo: DataRepository, year: Optional[int]) -> pd.DataFrame:
    """
    Returns a dataframe grouped by nationality with total workers for a given year.
    Expected columns in labour_master: ['year', 'nationality', 'total_workers']
    """
    df = safe_get_df(repo, "labour_master")
    if df.empty:
        return pd.DataFrame()

    df = _safe_filter_year(df, year)
    if df.empty:
        return pd.DataFrame()

    if "nationality" not in df.columns or "total_workers" not in df.columns:
        return pd.DataFrame()

    grouped = df.groupby("nationality", as_index=False)["total_workers"].sum()
    return grouped.sort_values("total_workers", ascending=False)


def get_top_occupations(
    repo: DataRepository, year: Optional[int], top_n: int = 5
) -> pd.DataFrame:
    """
    Returns top N occupations by number of workers for a given year.
    Expected columns in occupation_workers:
        ['year', 'main_occupation', 'nationality', 'occupation_workers']
    """
    df = safe_get_df(repo, "occupation_workers")
    if df.empty:
        return pd.DataFrame()

    df = _safe_filter_year(df, year)
    if df.empty:
        return pd.DataFrame()

    if "main_occupation" not in df.columns:
        return pd.DataFrame()

    value_col = "occupation_workers" if "occupation_workers" in df.columns else None
    if value_col is None:
        return pd.DataFrame()

    df_sorted = df.sort_values(value_col, ascending=False)
    return df_sorted.head(top_n)[["main_occupation", value_col]]


# ---------- Households & Population Density ----------


def get_households_by_governorate(repo: DataRepository) -> pd.DataFrame:
    """
    Group households by governorate and nationality (if available).
    Expected columns: ['governorate', 'nationality', 'households']
    """
    df = safe_get_df(repo, "households")
    if df.empty:
        return pd.DataFrame()

    if "governorate" not in df.columns:
        return pd.DataFrame()

    value_col = "households" if "households" in df.columns else None
    if value_col is None:
        return pd.DataFrame()

    group_cols = ["governorate"]
    if "nationality" in df.columns:
        group_cols.append("nationality")

    grouped = df.groupby(group_cols, as_index=False)[value_col].sum()
    return grouped.sort_values(value_col, ascending=False)


def get_latest_density(repo: DataRepository) -> pd.DataFrame:
    """
    Returns population density info for the latest available year.
    Expected columns: ['year', 'governorate', 'population', 'density'] (density optional).
    """
    df = safe_get_df(repo, "population_density")
    if df.empty or "year" not in df.columns:
        return pd.DataFrame()

    latest_year = df["year"].max()
    df_latest = df[df["year"] == latest_year]

    return df_latest


# ---------- Housing Units ----------


def get_housing_units_summary(repo: DataRepository) -> pd.DataFrame:
    """
    Group housing units by governorate and housing_type if available.
    Expected columns: ['governorate', 'housing_type', 'units']
    """
    df = safe_get_df(repo, "housing_units")
    if df.empty:
        return pd.DataFrame()

    if "governorate" not in df.columns:
        return pd.DataFrame()

    value_col = "units" if "units" in df.columns else None
    if value_col is None:
        return pd.DataFrame()

    group_cols = ["governorate"]
    if "housing_type" in df.columns:
        group_cols.append("housing_type")

    grouped = df.groupby(group_cols, as_index=False)[value_col].sum()
    return grouped.sort_values(value_col, ascending=False)


# ---------- Students ----------


def get_students_summary(repo: DataRepository) -> pd.DataFrame:
    """
    Group students by sector and level if available.
    Expected columns: ['year', 'sector', 'level', 'students']
    """
    df = safe_get_df(repo, "students")
    if df.empty:
        return pd.DataFrame()

    value_col = "students" if "students" in df.columns else None
    if value_col is None:
        return pd.DataFrame()

    group_cols = []
    for c in ["sector", "level", "school_type"]:
        if c in df.columns:
            group_cols.append(c)

    if not group_cols:
        return pd.DataFrame()

    grouped = df.groupby(group_cols, as_index=False)[value_col].sum()
    return grouped.sort_values(value_col, ascending=False)


# ---------- Teachers ----------


def get_teachers_summary(repo: DataRepository) -> pd.DataFrame:
    """
    Group teachers by school_type and level if available.
    Expected columns: ['year', 'school_type', 'level', 'teachers']
    """
    df = safe_get_df(repo, "teachers")
    if df.empty:
        return pd.DataFrame()

    value_col = "teachers" if "teachers" in df.columns else None
    if value_col is None:
        return pd.DataFrame()

    group_cols = []
    for c in ["school_type", "level", "sector"]:
        if c in df.columns:
            group_cols.append(c)

    if not group_cols:
        return pd.DataFrame()

    grouped = df.groupby(group_cols, as_index=False)[value_col].sum()
    return grouped.sort_values(value_col, ascending=False)


# ---------- Higher Education ----------


def get_higher_education_summary(repo: DataRepository) -> pd.DataFrame:
    """
    Group higher education students by sector and academic_programme.
    Expected columns: ['year', 'sector', 'academic_programme', 'students']
    This version is defensive: if expected columns are missing it attempts reasonable fallbacks,
    and always avoids passing invalid (empty) group lists to pandas.
    """
    df = safe_get_df(repo, "higher_education")
    if df.empty:
        return pd.DataFrame()

    # try common column names (normalized by data_layer)
    value_col = None
    for cand in ["students", "value", "n", "count"]:
        if cand in df.columns:
            value_col = cand
            break
    if value_col is None:
        # nothing numeric to aggregate â€” return a small schema-only DataFrame
        return pd.DataFrame({"note": ["no numeric student column found"], "available_columns": [", ".join(df.columns)]})

    # choose group columns from likely candidates, but only those actually present
    group_cols = [c for c in ["sector", "academic_programme", "level", "year"] if c in df.columns]

    # If nothing sensible to group by, return overall aggregates
    try:
        if not group_cols:
            s = pd.to_numeric(df[value_col], errors="coerce").dropna()
            if s.empty:
                return pd.DataFrame()
            agg = {
                "total_students": float(s.sum()),
                "median_students": float(s.median()),
                "n_records": int(s.count()),
            }
            return pd.DataFrame([agg])

        # Otherwise group safely
        grouped = df.groupby(group_cols, as_index=False)[value_col].sum()
        # sort by value if possible
        grouped = grouped.sort_values(by=value_col, ascending=False)
        return grouped
    except Exception:
        # Very defensive fallback: return a safe message DataFrame
        return pd.DataFrame({"error": ["failed to aggregate higher_education"], "available_columns": [", ".join(df.columns)]})


# --------------------------------------------------------------------
# ---------- New functions for Domestic Worker & Related datasets -----
# --------------------------------------------------------------------


def get_domestic_permit_stats(repo, year: Optional[int] = None) -> pd.DataFrame:
    """
    Aggregate domestic worker permit statistics.

    Looks for a repo attribute in this order (heuristic):
      - "domestic_permits"
      - "domestic_worker_permits"
      - "permits_domestic"

    Expected useful columns (any subset):
      year, status, permit_type, governorate/area_name, nationality, job_role,
      active_permits, new_permits, cancelled_permits, renewed_permits

    Returns a multi-index (or flat) dataframe with numeric aggregates per grouping.
    """
    # pick best candidate layer
    candidates = ["domestic_permits", "domestic_worker_permits", "permits_domestic"]
    df = pd.DataFrame()
    for c in candidates:
        df_try = safe_get_df(repo, c)
        if not df_try.empty:
            df = df_try
            break
    if df.empty:
        return pd.DataFrame()

    df = _safe_filter_year(df, year)
    if df.empty:
        return pd.DataFrame()

    # normalize common columns
    possible_area = "governorate" if "governorate" in df.columns else ("area_name" if "area_name" in df.columns else None)
    possible_job_role = (
        "job_role" if "job_role" in df.columns else ("occupation" if "occupation" in df.columns else None)
    )
    possible_nationality = "nationality" if "nationality" in df.columns else None

    # numeric fields - try to detect
    numeric_candidates = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
    # common logical fields
    results = {}

    # Provide summary for statuses if available
    if "status" in df.columns:
        try:
            status_group = df.groupby("status").size().reset_index(name="count")
            results["by_status"] = status_group
        except Exception:
            pass

    # Aggregates by area/governorate
    if possible_area:
        val_cols = [c for c in ["active_permits", "new_permits", "cancelled_permits", "renewed_permits"] if c in df.columns]
        if not val_cols:
            # fallback: count rows per area
            counts = df.groupby(possible_area).size().reset_index(name="rows")
            results["by_area_counts"] = counts.sort_values("rows", ascending=False)
        else:
            grouped = df.groupby(possible_area, as_index=False)[val_cols].sum()
            results["by_area"] = grouped.sort_values(val_cols[0], ascending=False)

    # Aggregates by nationality
    if possible_nationality:
        val_cols = [c for c in ["active_permits", "new_permits", "cancelled_permits", "renewed_permits"] if c in df.columns]
        if not val_cols:
            counts = df.groupby(possible_nationality).size().reset_index(name="rows")
            results["by_nationality_counts"] = counts.sort_values("rows", ascending=False)
        else:
            grouped = df.groupby(possible_nationality, as_index=False)[val_cols].sum()
            results["by_nationality"] = grouped.sort_values(val_cols[0], ascending=False)

    # Aggregates by job role
    if possible_job_role:
        val_cols = [c for c in ["active_permits", "new_permits", "cancelled_permits", "renewed_permits"] if c in df.columns]
        if not val_cols:
            counts = df.groupby(possible_job_role).size().reset_index(name="rows")
            results["by_job_role_counts"] = counts.sort_values("rows", ascending=False)
        else:
            grouped = df.groupby(possible_job_role, as_index=False)[val_cols].sum()
            results["by_job_role"] = grouped.sort_values(val_cols[0], ascending=False)

    # If there are numeric summary columns, attach a numeric summary
    if numeric_candidates:
        numeric_summary = {}
        for c in numeric_candidates:
            s = ensure_numeric_series(df[c])
            numeric_summary[c] = {"count": int(s.count()), "sum": float(s.sum()) if s.count() else 0.0}
        results["numeric_summary"] = numeric_summary

    # Return a combined representation: prefer a single DataFrame if only one key, else return concatenated key-indexed frames
    if not results:
        return pd.DataFrame()
    if len(results) == 1:
        return list(results.values())[0]
    # flatten: create a multi-sheet-like single DF by adding a _section column
    frames = []
    for k, v in results.items():
        if isinstance(v, pd.DataFrame):
            tmp = v.copy()
            tmp["_section"] = k
            frames.append(tmp)
    if frames:
        out = pd.concat(frames, ignore_index=True, sort=False)
        # put _section first
        cols = ["_section"] + [c for c in out.columns if c != "_section"]
        return out[cols]
    # fallback: return empty DF
    return pd.DataFrame()


def get_employer_household_stats(repo, year: Optional[int] = None) -> pd.DataFrame:
    """
    Employer household statistics:
      - number of households hiring domestic workers
      - average domestic workers per household
      - households with 2+ or 3+ workers
      - breakdown by area/governorate

    Looks for layers: "employer_households", "households_employing_workers", "household_domestic"
    """
    candidates = ["employer_households", "households_employing_workers", "household_domestic"]
    df = pd.DataFrame()
    for c in candidates:
        df_try = safe_get_df(repo, c)
        if not df_try.empty:
            df = df_try
            break
    if df.empty:
        return pd.DataFrame()

    df = _safe_filter_year(df, year)
    if df.empty:
        return pd.DataFrame()

    # expected columns: household_id (or household), n_domestic_workers, governorate/area_name
    worker_col = "n_domestic_workers" if "n_domestic_workers" in df.columns else (
        "domestic_workers" if "domestic_workers" in df.columns else None
    )
    household_id = "household_id" if "household_id" in df.columns else ("household" if "household" in df.columns else None)
    area_col = "governorate" if "governorate" in df.columns else ("area_name" if "area_name" in df.columns else None)

    if worker_col is None and household_id is None:
        return pd.DataFrame()

    # Average workers per household
    if worker_col:
        df[worker_col] = ensure_numeric_series(df[worker_col]).fillna(0)
        avg = df[worker_col].mean()
        total_households = df[household_id].nunique() if household_id else len(df)
        households_2plus = df[df[worker_col] >= 2][household_id].nunique() if household_id else int((df[worker_col] >= 2).sum())
        households_3plus = df[df[worker_col] >= 3][household_id].nunique() if household_id else int((df[worker_col] >= 3).sum())
        summary = {
            "avg_domestic_workers_per_household": float(avg),
            "total_households_counted": int(total_households),
            "households_with_2_plus": int(households_2plus),
            "households_with_3_plus": int(households_3plus),
        }
    else:
        # fallback: estimate from household rows (presence = 1)
        total_households = df[household_id].nunique() if household_id else len(df)
        summary = {"total_households_counted": int(total_households)}

    # By area
    area_df = pd.DataFrame()
    if area_col:
        if worker_col:
            area_df = df.groupby(area_col, as_index=False)[worker_col].agg(["sum", "mean", "count"]).reset_index()
            area_df.columns = [area_col, "workers_sum", "workers_mean", "household_rows"]
        else:
            area_df = df.groupby(area_col, as_index=False).size().reset_index(name="household_rows")

    # build final output: first row summary, then area breakdown (if any)
    summary_df = pd.DataFrame([summary])
    if not area_df.empty:
        area_df["_section"] = "by_area"
        summary_df["_section"] = "summary"
        out = pd.concat([summary_df, area_df], ignore_index=True, sort=False)
        cols = ["_section"] + [c for c in out.columns if c != "_section"]
        return out[cols]
    return summary_df

def get_households_without_domestic_workers(repo, year: Optional[int] = None) -> pd.DataFrame:
    """
    Return households (household_id / household rows) that do NOT appear to employ a domestic worker.
    Heuristics (in descending preference):
      1) If a household-employer mapping exists (layer names: employer_households, households_employing_workers, household_domestic),
         use household_id values and find those not present in employer list.
      2) If domestic_permits (or similar) contains employer_id/household_id, use that to identify households with workers.
      3) If none of the above exist, attempt a fallback estimate: if employer_households not found, but households exists,
         return households with n_domestic_workers == 0 (if that column exists).
    The function never mutates repo frames and returns an empty DataFrame if insufficient data.
    """
    # 1) try explicit employer-household mapping
    candidates_employer = ["employer_households", "households_employing_workers", "household_domestic"]
    emp_df = pd.DataFrame()
    for c in candidates_employer:
        tmp = safe_get_df(repo, c)
        if not tmp.empty:
            emp_df = tmp
            break

    # 2) try domestic permits (may have employer_id or household_id)
    permits_candidates = ["domestic_permits", "domestic_worker_permits", "permits_domestic"]
    perm_df = pd.DataFrame()
    for c in permits_candidates:
        tmp = safe_get_df(repo, c)
        if not tmp.empty:
            perm_df = tmp
            break

    # 3) main households table
    households = safe_get_df(repo, "households")
    if households.empty:
        # nothing to compare against
        return pd.DataFrame()

    # filter by year if possible
    if year is not None:
        households = _safe_filter_year(households, year)
        if not emp_df.empty:
            emp_df = _safe_filter_year(emp_df, year)
        if not perm_df.empty:
            perm_df = _safe_filter_year(perm_df, year)

    # prefer employer-household mapping first
    if not emp_df.empty:
        # find plausible household id column names
        hh_col = None
        for cand in ("household_id", "hh_id", "household"):
            if cand in emp_df.columns:
                hh_col = cand
                break
        if hh_col is not None:
            employed_hh_ids = emp_df[hh_col].dropna().astype(str).unique().tolist()
            # ensure households has a comparable id column; try to detect
            target_hh_col = None
            for cand in ("household_id", "hh_id", "household"):
                if cand in households.columns:
                    target_hh_col = cand
                    break
            if target_hh_col:
                # compute households without employer ids
                mask = ~households[target_hh_col].astype(str).isin([str(x) for x in employed_hh_ids])
                return households[mask].copy()
            else:
                # households doesn't have id column; fallback to returning empty
                return pd.DataFrame()

    # next fallback: use permits/employer_id
    if not perm_df.empty:
        # detect employer id column in permits
        empid_col = None
        for cand in ("employer_id", "employer", "household_id", "permit_holder", "holder_id"):
            if cand in perm_df.columns:
                empid_col = cand
                break
        # detect household id in households
        target_hh_col = None
        for cand in ("household_id", "hh_id", "household"):
            if cand in households.columns:
                target_hh_col = cand
                break
        if empid_col and target_hh_col:
            employed_hh_ids = perm_df[empid_col].dropna().astype(str).unique().tolist()
            mask = ~households[target_hh_col].astype(str).isin([str(x) for x in employed_hh_ids])
            return households[mask].copy()

    # last fallback: if households table itself has a numeric column counting domestic workers
    for cand in ("num_domestic_workers", "n_domestic_workers", "domestic_workers_count", "domestic_workers"):
        if cand in households.columns:
            try:
                s = ensure_numeric_series(households[cand]).fillna(0)
                return households[s == 0].copy()
            except Exception:
                break

    # if we reach here, we don't have a reliable way to determine non-employer households
    return pd.DataFrame()


def get_work_permit_applications(repo, year: Optional[int] = None) -> pd.DataFrame:
    """
    Work permit applications summary:
      - total applications
      - counts by status (pending / approved / rejected)
      - breakdown by area, nationality, job role

    Looks for layers like "work_permit_applications", "permit_applications", "applications"
    """
    candidates = ["work_permit_applications", "permit_applications", "applications"]
    df = pd.DataFrame()
    for c in candidates:
        df_try = safe_get_df(repo, c)
        if not df_try.empty:
            df = df_try
            break
    if df.empty:
        return pd.DataFrame()

    df = _safe_filter_year(df, year)
    if df.empty:
        return pd.DataFrame()

    results = {}

    # total applications
    total = len(df)
    results["total_applications"] = pd.DataFrame([{"total_applications": int(total)}])

    # by status
    if "status" in df.columns:
        status_df = df["status"].value_counts(dropna=False).rename_axis("status").reset_index(name="count")
        results["by_status"] = status_df

    # by area
    area_col = "governorate" if "governorate" in df.columns else ("area_name" if "area_name" in df.columns else None)
    if area_col:
        area_df = df.groupby(area_col).size().reset_index(name="applications")
        results["by_area"] = area_df.sort_values("applications", ascending=False)

    # by nationality
    if "nationality" in df.columns:
        nat_df = df.groupby("nationality").size().reset_index(name="applications")
        results["by_nationality"] = nat_df.sort_values("applications", ascending=False)

    # by job role
    job_col = "job_role" if "job_role" in df.columns else ("occupation" if "occupation" in df.columns else None)
    if job_col:
        job_df = df.groupby(job_col).size().reset_index(name="applications")
        results["by_job_role"] = job_df.sort_values("applications", ascending=False)

    # Merge results into one DF if convenient
    frames = []
    for k, v in results.items():
        if isinstance(v, pd.DataFrame):
            tmp = v.copy()
            tmp["_section"] = k
            frames.append(tmp)

    if frames:
        out = pd.concat(frames, ignore_index=True, sort=False)
        cols = ["_section"] + [c for c in out.columns if c != "_section"]
        return out[cols]
    return pd.DataFrame()


def get_permit_renewals(repo, year: Optional[int] = None) -> pd.DataFrame:
    """
    Permit renewals summary:
      - counts of renewals
      - renewal rates (renewals / eligible)
      - breakdown by area
    Expects a layer like "permit_renewals" or "renewals" with columns:
      year, renewed (bool or status), eligible_count, governorate
    """
    candidates = ["permit_renewals", "renewals"]
    df = pd.DataFrame()
    for c in candidates:
        df_try = safe_get_df(repo, c)
        if not df_try.empty:
            df = df_try
            break
    if df.empty:
        return pd.DataFrame()

    df = _safe_filter_year(df, year)
    if df.empty:
        return pd.DataFrame()

    # attempt to find fields
    renewed_col = None
    for cand in ["renewed", "renewal", "renewal_count", "renewed_count"]:
        if cand in df.columns:
            renewed_col = cand
            break
    eligible_col = None
    for cand in ["eligible_count", "eligible", "eligible_pool"]:
        if cand in df.columns:
            eligible_col = cand
            break

    # create basic summary
    if renewed_col:
        s = ensure_numeric_series(df[renewed_col]).fillna(0)
        total_renewed = int(s.sum())
    else:
        total_renewed = None

    if eligible_col:
        e = ensure_numeric_series(df[eligible_col]).fillna(0)
        total_eligible = int(e.sum())
    else:
        total_eligible = None

    renewal_rate = None
    if total_renewed is not None and total_eligible:
        renewal_rate = float(total_renewed) / float(total_eligible) if total_eligible > 0 else None

    summary = {
        "total_renewed": total_renewed,
        "total_eligible": total_eligible,
        "renewal_rate": renewal_rate,
    }
    summary_df = pd.DataFrame([summary])

    # by area
    area_col = "governorate" if "governorate" in df.columns else ("area_name" if "area_name" in df.columns else None)
    area_df = pd.DataFrame()
    if area_col:
        cols_to_sum = []
        if renewed_col:
            cols_to_sum.append(renewed_col)
        if eligible_col:
            cols_to_sum.append(eligible_col)
        if cols_to_sum:
            area_df = df.groupby(area_col, as_index=False)[cols_to_sum].sum()
            # compute area-level rate if possible
            if renewed_col in area_df.columns and eligible_col in area_df.columns:
                area_df["renewal_rate"] = area_df[renewed_col] / area_df[eligible_col].replace({0: pd.NA})
        else:
            area_df = df.groupby(area_col).size().reset_index(name="rows")

    if not area_df.empty:
        area_df["_section"] = "by_area"
        summary_df["_section"] = "summary"
        out = pd.concat([summary_df, area_df], ignore_index=True, sort=False)
        cols = ["_section"] + [c for c in out.columns if c != "_section"]
        return out[cols]
    return summary_df


def get_workforce_distribution(repo, year: Optional[int] = None) -> pd.DataFrame:
    """
    Distribution of domestic workforce across sectors:
      - cleaning companies, hospitality/event, catering/housekeeping, etc.
      - by area where available

    Looks for layers: "workforce", "domestic_workforce", "workers_by_company"
    """
    candidates = ["workforce", "domestic_workforce", "workers_by_company", "workers_distribution"]
    df = pd.DataFrame()
    for c in candidates:
        df_try = safe_get_df(repo, c)
        if not df_try.empty:
            df = df_try
            break
    if df.empty:
        return pd.DataFrame()

    df = _safe_filter_year(df, year)
    if df.empty:
        return pd.DataFrame()

    # expected columns: sector/company_type, governorate/area_name, workers_count
    sector_col = None
    for cand in ["sector", "company_type", "industry", "business_type"]:
        if cand in df.columns:
            sector_col = cand
            break
    workers_col = None
    for cand in ["workers", "workers_count", "num_workers", "staff_count"]:
        if cand in df.columns:
            workers_col = cand
            break
    area_col = "governorate" if "governorate" in df.columns else ("area_name" if "area_name" in df.columns else None)

    if sector_col and workers_col:
        df[workers_col] = ensure_numeric_series(df[workers_col]).fillna(0)
        sec_df = df.groupby(sector_col, as_index=False)[workers_col].sum().sort_values(workers_col, ascending=False)
        # also provide breakdown by area if possible
        if area_col:
            area_df = df.groupby([area_col, sector_col], as_index=False)[workers_col].sum()
            # flatten
            area_df["_section"] = "by_area_and_sector"
            sec_df["_section"] = "by_sector"
            out = pd.concat([sec_df, area_df], ignore_index=True, sort=False)
            cols = ["_section"] + [c for c in out.columns if c != "_section"]
            return out[cols]
        sec_df["_section"] = "by_sector"
        cols = ["_section"] + [c for c in sec_df.columns if c != "_section"]
        return sec_df[cols]
    # fallback: counts
    if sector_col:
        counts = df.groupby(sector_col).size().reset_index(name="rows")
        counts["_section"] = "by_sector_counts"
        return counts[["_section"] + [c for c in counts.columns if c != "_section"]]
    return pd.DataFrame()


def get_salary_fee_aggregates(repo, year: Optional[int] = None) -> pd.DataFrame:
    """
    Aggregates for salary and recruitment fees:
      - average salary, recruitment fee averages
      - salary brackets (if salary column present)
    Expects layer names like "domestic_salary", "salary_fees", "salary_data"
    """
    candidates = ["domestic_salary", "salary_fees", "salary_data", "worker_salaries"]
    df = pd.DataFrame()
    for c in candidates:
        df_try = safe_get_df(repo, c)
        if not df_try.empty:
            df = df_try
            break
    if df.empty:
        return pd.DataFrame()

    df = _safe_filter_year(df, year)
    if df.empty:
        return pd.DataFrame()

    salary_col = None
    for cand in ["salary", "avg_salary", "monthly_salary", "wage"]:
        if cand in df.columns:
            salary_col = cand
            break
    fee_col = None
    for cand in ["recruitment_fee", "recruitment_fees", "fee"]:
        if cand in df.columns:
            fee_col = cand
            break

    results = {}
    if salary_col:
        s = ensure_numeric_series(df[salary_col]).dropna()
        if not s.empty:
            results["salary_aggregates"] = pd.DataFrame(
                [
                    {
                        "avg_salary": float(s.mean()),
                        "median_salary": float(s.median()),
                        "min_salary": float(s.min()),
                        "max_salary": float(s.max()),
                        "n_salary_records": int(s.count()),
                    }
                ]
            )
            # salary brackets (simple quantiles)
            q = s.quantile([0, 0.25, 0.5, 0.75, 1.0]).to_dict()
            brackets = pd.DataFrame([{"quantile": k, "value": float(v)} for k, v in q.items()])
            brackets["_section"] = "salary_brackets"
            results["salary_brackets"] = brackets

    if fee_col:
        f = ensure_numeric_series(df[fee_col]).dropna()
        if not f.empty:
            results["fee_aggregates"] = pd.DataFrame(
                [
                    {
                        "avg_fee": float(f.mean()),
                        "median_fee": float(f.median()),
                        "min_fee": float(f.min()),
                        "max_fee": float(f.max()),
                        "n_fee_records": int(f.count()),
                    }
                ]
            )

    # combine
    frames = []
    for k, v in results.items():
        if isinstance(v, pd.DataFrame):
            tmp = v.copy()
            tmp["_section"] = k
            frames.append(tmp)
    if frames:
        out = pd.concat(frames, ignore_index=True, sort=False)
        cols = ["_section"] + [c for c in out.columns if c != "_section"]
        return out[cols]
    return pd.DataFrame()


# --------------------------------------------------------------------
# ---------- Footfall / Movement / Mobility & Demographic signals -----
# --------------------------------------------------------------------


def get_footfall_summary(repo, area: Optional[str] = None, period: Optional[str] = None) -> pd.DataFrame:
    """
    Summarize footfall / movement patterns.

    Expects layers such as:
      "footfall", "movement", "mobility", "footfall_data"

    Expected columns: area_name/governorate, area_code, datetime/date, footfall_count,
                     dwell_time, movement_direction, unique_devices

    Returns aggregated counts by area (or filtered area) and period if present.
    """
    candidates = ["footfall", "movement", "mobility", "footfall_data"]
    df = pd.DataFrame()
    for c in candidates:
        df_try = safe_get_df(repo, c)
        if not df_try.empty:
            df = df_try
            break
    if df.empty:
        return pd.DataFrame()

    # optional filters
    if area:
        if "area_name" in df.columns:
            df = df[df["area_name"] == area]
        elif "governorate" in df.columns:
            df = df[df["governorate"] == area]
    # period is a loose filter (e.g., "2024-01" or a custom string); we try 'period' column first
    if period and "period" in df.columns:
        df = df[df["period"] == period]

    # typical numeric columns
    count_col = None
    for cand in ["footfall_count", "count", "visits", "footfall_visits"]:
        if cand in df.columns:
            count_col = cand
            break
    dwell_col = None
    for cand in ["dwell_time", "avg_dwell", "dwell"]:
        if cand in df.columns:
            dwell_col = cand
            break
    unique_col = None
    for cand in ["unique_devices", "unique_count", "devices"]:
        if cand in df.columns:
            unique_col = cand
            break

    area_col = "area_name" if "area_name" in df.columns else ("governorate" if "governorate" in df.columns else None)
    # aggregation
    agg_cols = {}
    if count_col:
        agg_cols[count_col] = "sum"
    if unique_col:
        agg_cols[unique_col] = "sum"
    if dwell_col:
        agg_cols[dwell_col] = "mean"

    if area_col and agg_cols:
        out = df.groupby(area_col).agg(agg_cols).reset_index()
        return out.sort_values(list(agg_cols.keys())[0], ascending=False)
    # if no area_col, try overall summary
    if agg_cols:
        overall = {}
        for k, agg in agg_cols.items():
            if agg == "sum":
                overall[k] = float(ensure_numeric_series(df[k]).sum())
            elif agg == "mean":
                overall[k] = float(ensure_numeric_series(df[k]).mean())
        return pd.DataFrame([overall])
    return pd.DataFrame()



